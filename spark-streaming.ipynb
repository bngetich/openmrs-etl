{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Stractured Streaming\n",
    "![alt text](pics/demo2.png )\n",
    "\n",
    "The motivation of this project is to provide ability of processing data in **real-time**\n",
    " from various sources like openmrs, eid, e.t.c\n",
    "\n",
    "https://spark.apache.org/docs/2.3.3/structured-streaming-kafka-integration.html#deploying\n",
    "\n",
    "https://mtpatter.github.io/bilao/notebooks/html/01-spark-struct-stream-kafka.html\n",
    "\n",
    "http://www.adaltas.com/en/2019/04/18/spark-streaming-data-pipelines-with-structured-streaming/\n",
    "\n",
    "## Set up Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.109:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Structured Streaming from Kafka</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4fa43efa58>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "spark = SparkSession.builder \\\n",
    "            .appName(\"Spark Structured Streaming from Kafka\") \\\n",
    "            .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.3\") \\\n",
    "            .config('spark.executor.memory', '10G')\\\n",
    "            .config('spark.driver.memory', '10G')\\\n",
    "            .config('spark.driver.maxResultSize', '10G')\\\n",
    "            .getOrCreate()\n",
    " \n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection to Kakfa\n",
    "A Kafka topic can be viewed as an infinite stream where data is retained for a configurable amount of time. The infinite nature of this stream means that when starting a new query, we have to first decide what data to read and where in time we are going to begin. At a high level, there are three choices:\n",
    "\n",
    "- earliest — start reading at the beginning of the stream. This excludes data that has already been deleted from Kafka because it was older than the retention period (“aged out” data).\n",
    "- latest — start now, processing only new data that arrives after the query has started.\n",
    "\n",
    "<img src=\"https://databricks.com/wp-content/uploads/2017/04/kafka-topic.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaStreamDF = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribePattern\", \"dbserver1.openmrs.*\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load() \n",
    "\n",
    "# for single topic==>   .option(\"subscribe\", \"topic\") \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create output for Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "None\n",
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "|key|value|topic|partition|offset|timestamp|timestampType|\n",
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "kafkaRaw = kafkaStreamDF \\\n",
    "        .writeStream \\\n",
    "        .queryName(\"kafkaraw\")\\\n",
    "        .format(\"memory\")\\\n",
    "        .start()\n",
    "\n",
    "raw = spark.sql(\"select * from kafkaraw\")\n",
    "print(raw.printSchema())\n",
    "print(raw.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cast Value as String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "DataFrame[value: string]\n"
     ]
    }
   ],
   "source": [
    "#ds = dsraw.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "ds = kafkaStreamDF.selectExpr(\"CAST(value AS STRING)\")\n",
    "print(type(ds))\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "castedDF = kafkaStreamDF.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "        .writeStream \\\n",
    "        .queryName(\"castedDFS\")\\\n",
    "        .format(\"memory\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "None\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"schema\":{\"type\"...|\n",
      "|{\"schema\":{\"type\"...|\n",
      "|{\"schema\":{\"type\"...|\n",
      "|{\"schema\":{\"type\"...|\n",
      "|{\"schema\":{\"type\"...|\n",
      "|{\"schema\":{\"type\"...|\n",
      "|{\"schema\":{\"type\"...|\n",
      "|{\"schema\":{\"type\"...|\n",
      "|{\"schema\":{\"type\"...|\n",
      "|{\"schema\":{\"type\"...|\n",
      "|{\"schema\":{\"type\"...|\n",
      "|{\"schema\":{\"type\"...|\n",
      "|{\"schema\":{\"type\"...|\n",
      "|{\"schema\":{\"type\"...|\n",
      "|{\"schema\":{\"type\"...|\n",
      "|{\"schema\":{\"type\"...|\n",
      "|{\"schema\":{\"type\"...|\n",
      "|{\"schema\":{\"type\"...|\n",
      "|{\"schema\":{\"type\"...|\n",
      "|{\"schema\":{\"type\"...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "castedDFS = spark.sql(\"select * from castedDFS\")\n",
    "print(castedDFS.printSchema())\n",
    "print(castedDFS.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert To Parquet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f4fa4184588>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "kafkaStreamDF.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "        .writeStream \\\n",
    "        .format(\"parquet\")\\\n",
    "        .option(\"path\",\"streamingdata\")\\\n",
    "        .option(\"checkpointLocation\", \"streamcheckpoint\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push To Kafka as new topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write key-value data from a DataFrame to a Kafka topic specified in an option\n",
    "query =kafkaStreamDF.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\\\n",
    "  .writeStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "  .option(\"topic\",\"spark_to_kafka\") \\\n",
    "  .option(\"checkpointLocation\", \"sinkcheckpoint\") \\\n",
    "  .start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
