{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Stractured Streaming\n",
    "![alt text](pics/demo2.png )\n",
    "\n",
    "The motivation of this project is to provide ability of processing data in **real-time**\n",
    " from various sources like openmrs, eid, e.t.c\n",
    "\n",
    "https://spark.apache.org/docs/2.3.3/structured-streaming-kafka-integration.html#deploying\n",
    "\n",
    "https://mtpatter.github.io/bilao/notebooks/html/01-spark-struct-stream-kafka.html\n",
    "\n",
    "http://www.adaltas.com/en/2019/04/18/spark-streaming-data-pipelines-with-structured-streaming/\n",
    "\n",
    "## Set up Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.109:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Structured Streaming from Kafka</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3de48f28d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "spark = SparkSession.builder \\\n",
    "            .appName(\"Spark Structured Streaming from Kafka\") \\\n",
    "            .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.3\") \\\n",
    "            .config('spark.executor.memory', '10G')\\\n",
    "            .config('spark.driver.memory', '10G')\\\n",
    "            .config('spark.driver.maxResultSize', '10G')\\\n",
    "            .getOrCreate()\n",
    " \n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection to Kakfa\n",
    "A Kafka topic can be viewed as an infinite stream where data is retained for a configurable amount of time. The infinite nature of this stream means that when starting a new query, we have to first decide what data to read and where in time we are going to begin. At a high level, there are three choices:\n",
    "\n",
    "- earliest — start reading at the beginning of the stream. This excludes data that has already been deleted from Kafka because it was older than the retention period (“aged out” data).\n",
    "- latest — start now, processing only new data that arrives after the query has started.\n",
    "\n",
    "<img src=\"https://databricks.com/wp-content/uploads/2017/04/kafka-topic.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "obs_schema = StructType([\n",
    "    StructField('obs_id', LongType(), True),\n",
    "    StructField('voided', BooleanType(), True),\n",
    "    StructField('concept_id', IntegerType(), True),\n",
    "    StructField('obs_datetime', TimestampType(), True),\n",
    "    StructField('value', StringType(), True),\n",
    "    StructField('value_type', StringType(), True),\n",
    "    StructField('obs_group_id', IntegerType(), True),\n",
    "    StructField('parent_concept_id', IntegerType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- parsed_value: struct (nullable = true)\n",
      " |    |-- schema: string (nullable = true)\n",
      " |    |-- payload: struct (nullable = true)\n",
      " |    |    |-- before: string (nullable = true)\n",
      " |    |    |-- after: struct (nullable = true)\n",
      " |    |    |    |-- patient_id: long (nullable = true)\n",
      " |    |    |    |-- date_created: long (nullable = true)\n",
      " |    |    |    |-- creator: long (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "patient= StructType([\n",
    "    StructField('patient_id', LongType(), True),\n",
    "    StructField('date_created', LongType(), True),\n",
    "    StructField('creator', LongType(), True)\n",
    "])\n",
    "\n",
    "schema= StructType([\n",
    "                StructField('schema', StringType()),\n",
    "                StructField('payload', \n",
    "                           StructType([\n",
    "                                StructField('before', StringType()),\n",
    "                                StructField('after', patient)\n",
    "                           ])\n",
    "                           )\n",
    "            ])\n",
    "\n",
    "jsonOptions = { \"timestampFormat\": \"yyyy-MM-dd'T'HH:mm:ss.sss'Z'\" }\n",
    "\n",
    "kafkaStreamDF = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"dbserver1.openmrs.patient\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\\\n",
    "    .select(f.from_json(f.col(\"value\").cast(\"string\"), schema, jsonOptions).alias(\"parsed_value\"))\n",
    "    \n",
    "kafkaStreamDF.createOrReplaceTempView(\"patients\")\n",
    "\n",
    "print(kafkaStreamDF.printSchema())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=\"\"\"\n",
    "        SELECT\n",
    "          WINDOW(FROM_UNIXTIME(parsed_value.payload.after.date_created/1000), \"1 hour\", \"10 minutes\") AS eventWindow,\n",
    "          parsed_value.payload.after.creator AS creator,\n",
    "          AVG(parsed_value.payload.after.patient_id) AS avgAge,\n",
    "          MIN(parsed_value.payload.after.patient_id) AS minAge,\n",
    "          MAX(parsed_value.payload.after.patient_id) AS maxAge\n",
    "        FROM\n",
    "          patients\n",
    "        GROUP BY\n",
    "          eventWindow,\n",
    "          creator\n",
    "        ORDER BY\n",
    "          eventWindow,\n",
    "          creator\n",
    "  \"\"\"\n",
    "    \n",
    "query = spark.sql(sql)\n",
    "# show results\n",
    "result = query.writeStream\\\n",
    "    .format(\"console\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .option(\"truncate\", \"false\")\\\n",
    "    .start()\n",
    "\n",
    "\n",
    "#result.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query= kafkaStreamDF\\\n",
    "    .select(\"parsed_value.payload.after.*\")\\\n",
    "    .writeStream \\\n",
    "    .format(\"console\")\\\n",
    "    .start()\n",
    "\n",
    "#query.awaitTermination()\n",
    "# pleasse see your terminal/console "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](pics/console.png )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
